## Deep Learning

Recall the Supervise Learning Model sketch, which was posted in the Gradient Descent post. Here it is again:


****The Normal Data input Dataset consists n features.****
$$\hat{y}=\sigma(b+w^Tx)$$


![Supervise Learning Outlines](../assets/images/supervised/ML-data-activation-network.svg)


A deep Learning Network, AKA Deep Neural Network, uses a similar block as the above, repeated many times and tied in a mesh architecture as depicted in Figure 1.

### Figure 1: Deep Neural Network














a meshed networklayered meshed  of 


like logistic regression, repeated a lot of times

Previous posts presented Machine Learning, and its computation element - it is depicted in Figure 1.


If there are many layers without an activation function, it is always computing a linear prediction function, no matters how layers the network has.


The case with no activation function is the linear regression - predict a price etc.



and Deep learning is an approach to machine learning characterized by deep stacks of computations. This depth of computation is what has enabled deep learning models to disentangle the kinds of complex and hierarchical patterns found in the most challenging real-world datasets.

Through their power and scalability neural networks have become the defining model of deep learning. Neural networks are composed of neurons, where each neuron individually performs only a simple computation. The power of a neural network comes instead from the complexity of the connections these neurons can form.
